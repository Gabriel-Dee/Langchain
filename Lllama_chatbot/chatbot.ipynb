{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f955dae-c12f-42e2-9024-4361ce6c289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os \n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f06ed11-88bb-4e8a-8ba6-6a42eafebc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an instance of ollama\n",
    "ollama_llm = Ollama(model = 'llama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38218dd5-4f25-4273-bfe8-d81ac8764537",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "Model = 'gpt-3.5-turbo'\n",
    "\n",
    "# creating an instance of gpt\n",
    "gpt_llm = ChatOpenAI(api_key = API_KEY,model= Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35e651e4-dfb5-4f3e-9f44-272f22b65ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama_llm.invoke('what is a bot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92e74c46-ccb6-44b3-8335-8b3eb1b4ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "# API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "# Model = 'gpt-3.5-turbo'\n",
    "# gpt_llm = ChatOpenAI(api_key = API_KEY,model= Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0cec66d-3293-46a3-aeec-045f45f06518",
   "metadata": {},
   "outputs": [
    {
     "ename": "ChunkedEncodingError",
     "evalue": "('Connection broken: IncompleteRead(20 bytes read, 70 more expected)', IncompleteRead(20 bytes read, 70 more expected))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/response.py:710\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/response.py:1076\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1076\u001b[0m chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_chunk(amt)\n\u001b[1;32m   1077\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[1;32m   1078\u001b[0m     chunk, decode_content\u001b[38;5;241m=\u001b[39mdecode_content, flush_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/response.py:1027\u001b[0m, in \u001b[0;36mHTTPResponse._handle_chunk\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# amt > self.chunk_left\u001b[39;00m\n\u001b[0;32m-> 1027\u001b[0m     returned_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39m_safe_read(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39m_safe_read(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# type: ignore[union-attr] # Toss the CRLF at the end of the chunk.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/http/client.py:640\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m amt:\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[0;31mIncompleteRead\u001b[0m: IncompleteRead(20 bytes read, 70 more expected)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/response.py:933\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m--> 933\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/response.py:1061\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BodyNotHttplibCompatible(\n\u001b[1;32m   1057\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBody should be http.client.HTTPResponse like. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt should have have an fp attribute which returns raw chunks.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1059\u001b[0m     )\n\u001b[0;32m-> 1061\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# Don't bother reading the body of a HEAD request.\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_response \u001b[38;5;129;01mand\u001b[39;00m is_response_to_head(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_response):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/response.py:727\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (HTTPException, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    726\u001b[0m     \u001b[38;5;66;03m# This includes IncompleteRead.\u001b[39;00m\n\u001b[0;32m--> 727\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProtocolError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection broken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# If no exception is thrown, we should avoid cleaning up\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;66;03m# unnecessarily.\u001b[39;00m\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection broken: IncompleteRead(20 bytes read, 70 more expected)', IncompleteRead(20 bytes read, 70 more expected))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ollama_llm\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhat is a bot\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/llms.py:276\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    274\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 276\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    277\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    278\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    279\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    280\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    281\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    282\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    283\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    284\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    285\u001b[0m         )\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    288\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/llms.py:633\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    627\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    631\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    632\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/llms.py:803\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    789\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    790\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    791\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    801\u001b[0m         )\n\u001b[1;32m    802\u001b[0m     ]\n\u001b[0;32m--> 803\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[1;32m    804\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    805\u001b[0m     )\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/llms.py:670\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    669\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    671\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/llms.py:657\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    649\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    656\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 657\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    658\u001b[0m                 prompts,\n\u001b[1;32m    659\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    660\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[1;32m    661\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    662\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    663\u001b[0m             )\n\u001b[1;32m    664\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    665\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    666\u001b[0m         )\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_community/llms/ollama.py:417\u001b[0m, in \u001b[0;36mOllama._generate\u001b[0;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m--> 417\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_stream_with_aggregation(\n\u001b[1;32m    418\u001b[0m         prompt,\n\u001b[1;32m    419\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    420\u001b[0m         images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[1;32m    421\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m    422\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    424\u001b[0m     )\n\u001b[1;32m    425\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_community/llms/ollama.py:326\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[0;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    319\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    324\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[1;32m    325\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[1;32m    328\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_community/llms/ollama.py:172\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[0;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    166\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    170\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    171\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[1;32m    173\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[1;32m    174\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    175\u001b[0m         api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/generate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    177\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/models.py:865\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;124;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;124;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \n\u001b[1;32m    860\u001b[0m \u001b[38;5;124;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    863\u001b[0m pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(\n\u001b[1;32m    866\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, decode_unicode\u001b[38;5;241m=\u001b[39mdecode_unicode\n\u001b[1;32m    867\u001b[0m ):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pending \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    870\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m pending \u001b[38;5;241m+\u001b[39m chunk\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/utils.py:571\u001b[0m, in \u001b[0;36mstream_decode_response_unicode\u001b[0;34m(iterator, r)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    570\u001b[0m decoder \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mgetincrementaldecoder(r\u001b[38;5;241m.\u001b[39mencoding)(errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m    572\u001b[0m     rv \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(chunk)\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rv:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/models.py:818\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 818\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ContentDecodingError(e)\n",
      "\u001b[0;31mChunkedEncodingError\u001b[0m: ('Connection broken: IncompleteRead(20 bytes read, 70 more expected)', IncompleteRead(20 bytes read, 70 more expected))"
     ]
    }
   ],
   "source": [
    "ollama_llm.invoke('what is a bot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fdf98b-94d9-425f-88ac-d2cb0f588519",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('Data/data.txt',encoding = 'utf-8')\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd4c53-31da-4916-ba8a-7ba79981fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7357af27-f158-4578-8db6-0c92300803ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "spliter = RecursiveCharacterTextSplitter(chunk_size = 200,chunk_overlap = 50)\n",
    "chunks = spliter.split_documents(document)\n",
    "chunks[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72ba358-eb74-4685-9976-cb07157d5b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_storage = FAISS.from_documents(chunks,OllamaEmbeddings(model=\"llama3\"))\n",
    "retriever = vector_storage.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b7ec6b-a179-4c38-a78a-0ce55433b271",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever.invoke('what is your suggestion on breastfeeding practises for a lactating mother')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
